{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'Hello, We are trying out TensorFlow in this Lab!'\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import os\n",
    "import matplotlib.pyplot as plt \n",
    "import random as ran \n",
    "import numpy as np \n",
    "\n",
    "hello = tf.constant('Hello, We are trying out TensorFlow in this Lab!') \n",
    "sess = tf.Session() \n",
    "print(sess.run(hello))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:75: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  return _inspect.getargspec(target)\n",
      "/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "  return f(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-3-7857c12fdfa2>:4: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From /anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From /anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From /anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "#Load MNIST dataset.\n",
    "warnings.filterwarnings(action='once')\n",
    "from tensorflow.examples.tutorials.mnist import input_data \n",
    "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1- Explain this portion of the code.\n",
    "\n",
    "### Answer: The above code downloads the data and saves it to the MNIST_data folder, and processes it so that data is in one hot encoded format and assign to mnist variable.\n",
    "\n",
    "\n",
    "## Q2- what is the usage of one_hot in read_data_sets() command\n",
    "\n",
    "##### mnist = input_data.read_data_sets('MNIST_data', one_hot=True) \n",
    "\n",
    "### Answer: One hot enables conversion of a categorical variable to number. So it will essentially create a vector of numbers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions that can define the size of train and test sets \n",
    "def TRAIN_SIZE(num): \n",
    "    print ('Total Training Images in Dataset = ' + \n",
    "           str(mnist.train.images.shape)) \n",
    "    print ('--------------------------------------------------') \n",
    "    x_train = mnist.train.images[:num,:] \n",
    "    print ('x_train Examples Loaded = ' \n",
    "           + str(x_train.shape)) \n",
    "    y_train = mnist.train.labels[:num,:] \n",
    "    print ('y_train Examples Loaded = ' \n",
    "           + str(y_train.shape)) \n",
    "    print('') \n",
    "    return x_train, y_train \n",
    "\n",
    "def TEST_SIZE(num): \n",
    "    print ('Total Test Examples in Dataset = ' \n",
    "           + str(mnist.test.images.shape)) \n",
    "    print ('--------------------------------------------------') \n",
    "    x_test = mnist.test.images[:num,:] \n",
    "    print ('x_test Examples Loaded = ' \n",
    "           + str(x_test.shape)) \n",
    "    y_test = mnist.test.labels[:num,:] \n",
    "    print ('y_test Examples Loaded = ' \n",
    "           + str(y_test.shape)) \n",
    "    return x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define some functions that will help us in future for plotting some graphs.\n",
    "def display_digit(num): \n",
    "    print (y_train[num])\n",
    "    label = y_train[num].argmax(axis=0) \n",
    "    image = x_train[num].reshape([28,28]) \n",
    "    plt.title('Example: %d Label: %d' % (num, label)) \n",
    "    plt.imshow(image, cmap=plt.get_cmap('gray_r')) \n",
    "    plt.show() \n",
    "\n",
    "def display_mult_flat(start, stop): \n",
    "    images = x_train[start].reshape([1,784]) \n",
    "    for i in range(start+1,stop): \n",
    "        images = np.concatenate((images, x_train[i].reshape([1,784]))) \n",
    "    plt.imshow(images, cmap=plt.get_cmap('gray_r')) \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Training Images in Dataset = (55000, 784)\n",
      "--------------------------------------------------\n",
      "x_train Examples Loaded = (55000, 784)\n",
      "y_train Examples Loaded = (55000, 10)\n",
      "\n",
      "Total Test Examples in Dataset = (10000, 784)\n",
      "--------------------------------------------------\n",
      "x_test Examples Loaded = (10000, 784)\n",
      "y_test Examples Loaded = (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "x_train, y_train = TRAIN_SIZE(55000)\n",
    "x_test, y_test = TEST_SIZE(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_digit(ran.randint(0, x_train.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Open a session\n",
    "sess = tf.Session()\n",
    "\n",
    "# Input and output \n",
    "x = tf.placeholder(tf.float32, shape=[None, 784]) # input image 28*28 = 784 \n",
    "y_ = tf.placeholder(tf.float32, shape=[None, 10]) #0-9 digits (10 classes) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model parameters \n",
    "W = tf.Variable(tf.zeros([784, 10])) \n",
    "b = tf.Variable(tf.zeros([10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The best output activation node for multiclass will be softmax which is implemented here\n",
    "y = tf.nn.softmax(tf.matmul(x,W) + b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3- It appears that tf.placeholder() , tf.Variable() and tf.constant() all are for assigning variables so what is the difference between them?  \n",
    "\n",
    "### Answer: \n",
    "### tf.placeholder does not need an initial value and it can be assigned at the run time with the feed_dict argument inside Session.run\n",
    "### For tf.variable, an initial value has to be assigned while declaring the variable. Variables are trained over time, whereas a placeholder is just an input data that doesn't change as the model is trained. This makes placeholders good for input images, and class labels for those images. And variables are apt for weights, biases etc. \n",
    "### tf.constant creates a constant tensor.  tf.constant supports arbitrary constants, not just uniform scalar Tensors. As tf.constant only embeds constant values in the graph, it does not support dynamic shapes based on other runtime Tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize variables\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "#Train the model\n",
    "LEARNING_RATE = 0.1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y), reduction_indices=[1])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "training = tf.train.GradientDescentOptimizer(LEARNING_RATE).minimize(cross_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q4- Write a few sentences about tf.train.GradientDescentOptimizer and tf.equal commands. What is the effect of tf.argmax().\n",
    "\n",
    "### Answer: \n",
    "### tf.train.GradientDescentOptimizer is an optimizer that implements the gradient descent algorithm. \n",
    "#### The major line of interest here is training = tf.train.GradientDescentOptimizer(0.01).minimize(error) where the training step is defined. It aims to minimise the value of the error Variable, which is defined earlier as the cross_entropy. The 0.01 is the LEARNING_RATE - each step it takes to try learn a better value.\n",
    "\n",
    "### tf.equal is an elementwise operator command. Returns the truth value of (x == y) element-wise\n",
    "#### tf.equal() determines if the element in the first tensor equals the one in the second. We get an array of bools (True and False)\n",
    "\n",
    "### tf.argmax() - Returns the index with the largest value across axes of a tensor.\n",
    "#### Example: tf.argmax(input, axis=None, name=None, dimension=None)\n",
    "#### input is a Tensor and axis describes which axis of the input Tensor to reduce across. For vectors, use axis = 0. Returns the index with the largest value across axis of a tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Repeat the training steps based on the number of epochs defined.\n",
    "TRAIN_STEPS = 250 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(TRAIN_STEPS+1):\n",
    "    sess.run(training, feed_dict={x: x_train, y_: y_train})\n",
    "    if i%100 == 0:\n",
    "        print('Training Step:' + str(i) + ' Accuracy = ' + \n",
    "        str(sess.run(accuracy, feed_dict={x: x_test, y_: y_test})) + ' Loss = ' + \n",
    "        str(sess.run(cross_entropy, {x: x_train, y_: y_train})))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q5- Explain this part of the code In the code, \n",
    "\n",
    "### Answer: \n",
    "### The model is trained TRAIN_STEPS times using the for loop. In each loop, the values of x_train and y_train are passed to gradient descent optimizer by calling training to optimize the prediction. The second piece in the code is printing loss and accuracy on testing data. In this piece, the code is printing accuracy and loss value on testing data for all the run where the i/100==0.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add new layer\n",
    "#We are going to add two hidden layers. Input layer has 784 nodes. First hidden layer has 100 nodes. \n",
    "#Second hidden layer has 30 nodes. The output layer has 10 nodes.\n",
    "\n",
    "# Input and output \n",
    "X = tf.placeholder(tf.float32, shape=[None, 784]) # input image 28*28 = 784 \n",
    "Y_ = tf.placeholder(tf.float32, shape=[None, 10]) #0-9 digits (10 classes) \n",
    "\n",
    "# Two hidden layers, output layer and their number of neurons (the last layer has 10 softmax neurons) \n",
    "L = 100 \n",
    "M = 30\n",
    "\n",
    "# tf.truncated_normal is a TensorFlow function that produces random values following the normal (Gaussian) distribution between -2*stddev and +2*stddev \n",
    "W1 = tf.Variable(tf.truncated_normal([784, L], stddev=0.1)) # 784 = 28 * 28 \n",
    "B1 = tf.Variable(tf.zeros([L])) \n",
    "W2 = tf.Variable(tf.truncated_normal([L, M], stddev=0.1)) \n",
    "B2 = tf.Variable(tf.zeros([M])) \n",
    "W3 = tf.Variable(tf.truncated_normal([M, 10], stddev=0.1)) \n",
    "B3 = tf.Variable(tf.zeros([10]))\n",
    "\n",
    "# The model \n",
    "XX = tf.reshape(X, [-1, 784]) \n",
    "Y1 = tf.nn.sigmoid(tf.matmul(XX, W1) + B1) \n",
    "Y2 = tf.nn.sigmoid(tf.matmul(Y1, W2) + B2) \n",
    "Ylogits = tf.matmul(Y2, W3) + B3 \n",
    "Y = tf.nn.softmax(Ylogits)\n",
    "\n",
    "# cross-entropy loss function (= -sum(Y_i * log(Yi)) ), normalised for batches of 100 images \n",
    "# TensorFlow provides the softmax_cross_entropy_with_logits function to avoid numerical stability \n",
    "# problems with log(0) which is NaN \n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=Ylogits, labels=Y_) \n",
    "cross_entropy = tf.reduce_mean(cross_entropy)*100 \n",
    "# accuracy of the trained model, between 0 (worst) and 1 (best) \n",
    "correct_prediction = tf.equal(tf.argmax(Y, 1), tf.argmax(Y_, 1)) \n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "#Training step and initialization \n",
    "# init \n",
    "init = tf.global_variables_initializer() \n",
    "sess = tf.Session() \n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.01 \n",
    "#cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y), reduction_indices=[1])) \n",
    "training = tf.train.GradientDescentOptimizer(LEARNING_RATE).minimize(cross_entropy) \n",
    "#correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1)) \n",
    "#accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32)) \n",
    "TRAIN_STEPS = 1000 \n",
    "\n",
    "for i in range(TRAIN_STEPS+1): \n",
    "    sess.run(training, feed_dict={X: x_train, Y_: y_train}) \n",
    "    if i%100 == 0: \n",
    "        print('Training Step:' + str(i) + ' Accuracy = ' + \n",
    "              str(sess.run(accuracy, feed_dict={X: x_test, Y_: y_test})) + ' Loss = ' + \n",
    "              str(sess.run(cross_entropy, {X: x_train, Y_: y_train})))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DropOut\n",
    "\n",
    "# Input and output \n",
    "X = tf.placeholder(tf.float32, shape=[None, 784]) # input image 28*28 = 784 \n",
    "Y_ = tf.placeholder(tf.float32, shape=[None, 10]) #0-9 digits (10 classes) \n",
    "pkeep = tf.placeholder(tf.float32) \n",
    "L = 100 \n",
    "M = 30 \n",
    "N = 60 \n",
    "O = 30 \n",
    "# tf.truncated_normal is a TensorFlow function that produces random values following the normal (Gaussian) distribution between -2*stddev and +2*stddev \n",
    "W1 = tf.Variable(tf.truncated_normal([784, L], stddev=0.1)) # 784 = 28 * 28 \n",
    "B1 = tf.Variable(tf.ones([L])/10) \n",
    "W2 = tf.Variable(tf.truncated_normal([L, M], stddev=0.1)) \n",
    "B2 = tf.Variable(tf.ones([M])/10) \n",
    "W3 = tf.Variable(tf.truncated_normal([M, 10], stddev=0.1)) \n",
    "B3 = tf.Variable(tf.zeros([10])) \n",
    "# The model \n",
    "XX = tf.reshape(X, [-1, 784]) \n",
    "Y1 = tf.nn.relu(tf.matmul(XX, W1) + B1) \n",
    "Y1d = tf.nn.dropout(Y1, pkeep) \n",
    "Y2 = tf.nn.relu(tf.matmul(Y1, W2) + B2) \n",
    "Y2d = tf.nn.dropout(Y2, pkeep) \n",
    "Ylogits = tf.matmul(Y2d, W3) + B3 \n",
    "Y = tf.nn.softmax(Ylogits)\n",
    "\n",
    "# cross-entropy loss function (= -sum(Y_i * log(Yi)) ), normalised for batches of 100 images \n",
    "# TensorFlow provides the softmax_cross_entropy_with_logits function to avoid numerical stability \n",
    "# problems with log(0) which is NaN \n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits_v2(logits=Ylogits, labels=Y_) \n",
    "cross_entropy = tf.reduce_mean(cross_entropy)*100\n",
    "\n",
    "# accuracy of the trained model, between 0 (worst) and 1 (best) \n",
    "correct_prediction = tf.equal(tf.argmax(Y, 1), tf.argmax(Y_, 1)) \n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32)) \n",
    "\n",
    "# init \n",
    "init = tf.global_variables_initializer() \n",
    "sess = tf.Session() \n",
    "sess.run(init) \n",
    "LEARNING_RATE = 0.01 \n",
    "\n",
    "#cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y), reduction_indices=[1])) \n",
    "training = tf.train.GradientDescentOptimizer(LEARNING_RATE).minimize(cross_entropy)\n",
    "#correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))\n",
    "#accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "TRAIN_STEPS = 1000\n",
    "for i in range(TRAIN_STEPS+1):\n",
    "    sess.run(training, feed_dict={X: x_train, Y_: y_train, pkeep: 0.75 }) \n",
    "    if i%100 == 0:\n",
    "        print('Training Step:' + str(i) + ' Accuracy = ' + \n",
    "          str(sess.run(accuracy, feed_dict={X: x_test, Y_: y_test, pkeep: 1.0})) + \n",
    "          ' Loss = ' + str(sess.run(cross_entropy, {X: x_train, Y_: y_train , pkeep: 0.75})))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3. Stochastic Gradient Descent\n",
    "\n",
    "#### As discussed in class, the best way of addressing the deep neural network is through making smaller batches of data. Please make batches of 200 data and design a deep neural network with four hidden layer, ReLU as hidden layer activation function and softmax as the output activation layer. Please consider dropout for all the layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_STEPS = 150\n",
    "batch_size = 200\n",
    "for epoch in range(TRAIN_STEPS):\n",
    "    batch_count = int(mnist.train.num_examples/batch_size)\n",
    "    for i in range(batch_count):\n",
    "        batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "        sess.run(training, feed_dict={X: batch_x, Y_: batch_y, pkeep: 0.75})\n",
    "    if epoch%100 == 0:\n",
    "        print (\"Epoch: \", epoch)\n",
    "        print (\"Accuracy: \", str(sess.run(accuracy, feed_dict={X: mnist.test.images, Y_: mnist.test.labels, pkeep: 1.0})))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
